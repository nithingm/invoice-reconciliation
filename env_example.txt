# Ollama Configuration
# Make sure Ollama is installed and running on your system
# Download from: https://ollama.ai/

# Ollama Server URL (default: localhost:11434)
OLLAMA_URL=http://localhost:11434

# Ollama Model to use (default: llama2)
# Available models depend on what you have pulled with: ollama pull <model_name>
OLLAMA_MODEL=llama2

# Optional: Configure Ollama generation parameters
OLLAMA_TEMPERATURE=0.1
OLLAMA_MAX_TOKENS=2000

# Optional: Alternative models you can use
# OLLAMA_MODEL=codellama
# OLLAMA_MODEL=llama2:13b
# OLLAMA_MODEL=llama2:7b 